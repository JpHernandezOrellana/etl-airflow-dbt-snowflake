# ğŸ› ï¸ ETL Pipeline con Airflow, dbt y Snowflake

Este repositorio contiene un pipeline de datos local orquestado con **Apache Airflow**, transformaciones con **dbt**, y almacenamiento en **Snowflake** como data warehouse.

## ğŸ“¦ Estructura del Proyecto

```
text
etl_airflow_dbt_snowflake/
â”œâ”€â”€ airflow.cfg                 # ConfiguraciÃ³n de Airflow
â”œâ”€â”€ airflow_snowflake_project/ # Proyecto dbt
â”œâ”€â”€ dags/                      # DAG de Airflow con BashOperators para ejecutar dbt
â”œâ”€â”€ requirements.txt           # Dependencias del entorno
â”œâ”€â”€ .gitignore                 # Exclusiones del repositorio
â””â”€â”€ venv_airflow/              # Entorno virtual (excluido del repo)
```


---

## ğŸš€ Â¿QuÃ© hace este proyecto?

- **Extrae, transforma y documenta datos en Snowflake** usando `dbt`.
- **Orquesta el flujo de trabajo** con Airflow de forma modular.
- **Genera documentaciÃ³n automÃ¡tica** de los modelos dbt.

---

## âš™ï¸ Requisitos

- Python 3.10+
- Snowflake (cuenta activa)
- Airflow 2.9+
- dbt-core 1.10+
- Entorno virtual (`venv`) para aislamiento

## Uso:

Configura tu perfil dbt (profiles.yml)

Activa y ejecuta el DAG llamado: dbt_snowflake_pipeline


Una vez ejecutado el DAG, puedes generar y visualizar la documentaciÃ³n de los modelos:



